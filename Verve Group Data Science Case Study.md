# Verve Group Data Science Case Study
### Tasks
1. Imagine that you were asked to use this dataset to build a classification model, with `gender` as the target. Look at the information we have given you and identify 3-5 potential problems you can see with the provided dataset that might make building a classification model difficult.
- Our task is to build a classification model that predicts the "gender" feature, which is a categorical feature that has exactly two values, which is also termed as a binary variable. Looking at the **distribution of the output "gender" feature**, we  can see that the **distribution of the two classes is imbalanced** - nearly 72% of the users are men and about 28% of the users are women. For this reason, we should be very careful what kind of performance metric we are choosing for our classification model. For instance, accuracy, would be a bad choice, since it simply calculates Correct Predictions√∑Total Predictions, which does not consider the two classes separately. Better choices that give more insight into the performance of the model would be Precision or Recall. Another thing that we also should guarantee is to split the training and testing data sets proportionally to the actual distribution to the classes, i.e. approx. 70% males and approx. 30% females in both training and testing datasets. Another solution would be to up-sample the minority class (i.e. randomly duplicate entries in the minority class) or to down-sample the majority class (i.e. randomly remove entries from the majority class). 
- Another aspect that should not be overlooked is the "device_name" column. **ML classification models work with numerical data**, so we can use numerical columns as features, as well as categorical features, if we encode them them into numerical values, for example with one-hot encoder. However the **"device name" columns contains strings, that are neither numerical, nor categorical**. For this reason it is impossible to build this feature into the model. We could use the information from there, as we create new feature, which is identifying for example, whether the device's name contains a male's or a female's name, or maybe cannot be identified at all. We can use some regular-expression rule-based strategy or take advantage of already existing datasets that have identified names as male's or females's (e.g. https://data.world/howarder/gender-by-name) However, we cannot use the "device_name" columns as it is. 
- Among the categorical variables "app_category" and "ad_category" there are classes that occur rarely (e.g. less than 1% of the time). If each categorical feature has a **large number of distinct values, the encoding yields a large number of additional features**. The curse of dimensionality is a problem that many machine learning algorithms face when there are too many features. In our case we have almost only categorical features (except for "intraction_with_app"), so this might be a problem. A solution might be to group similar classes (e.g. in ad_category to group mid-range cars and luxury cars). 
- Also, among the **training variables there is a highly imbalanced one: the "click" variable**. Less than 0.5% of the users click on the ad. This may lead to **overfitting**. Let us assume that those 18 people that clicked on an ad are only males. The model will then automatically assume that if a person clicks on an ad, then it is a male, i.e. it overfits the data. This would be wrong, since only 0.5% actually clicked on an ad, therefore it should not be conclusive. Depending on the acutal distribution of the "click" feature, we may decide to omit the feature.
- Another problem that we come across are **missing values**. From the summary statistics, we can see that three of the non-numerical columns contain missing values: "device_name" with almost 60%; "app_category" and "ad_category" with about 1.8% (66 entries). As already discussed, we are not able utilize the "device_name" feature in the model. For this columns we can simply create a new column, which categorizes data into three classes: "female", "male" and "unknown", as we set "female" or "male" for all the entries that we can figure out, and for the ones that we cannot figure out or are missing we set the class "unknown". Since a user can have events for different apps, it is possible that the "device_name" is present for one app's interaction, and missing for the other. The ones that are missing can be filled as we map the "user_id" feature. The other two features are both missing exactly 66 entries. This most probably means that their missing values are occuring in the same rows, meaning that when the "app_category" is missing, the "ad_category" is also missing. There are different methods how two handle missing values. One option would be to delete the missing values. In our case this is also feasible, since the proportion of missing values is not that high. Another option would be to try to predict the missing values or replace them again with a new category (e.g. "unknown"). For the "interaction_with_app" feature, we can see from the histogram, that there are about 160 values, that are equal to 0. Those could be either entries that took less than 1 minute, or are missing values. In case they are missing values, we can use an imputation strategy to fill them (i.e. mean/median imputation) or delete them.  

2. Describe briefly how you would find the features that are likely to be the most important for your model. 
- In order to find out which features are likely to be important with to our model, we can **isolate the values** (male vs. female) and see what are the differences between genders **juxtaposed to the other features** (e.g. females spend their time mostly on "Beauty" apps, where as males spend their time mostly on "Sports" apps). We could also observe the **correlation** between the target variable "gender" with the other features. If the absolute value of the correlation coefficient is close to 1, this means that the feature is important for the prediction of the target variable "gender". 

3. Identify which model you would try first, and at least one advantage and disadvantage of this choice. 
- In our case we have access to the target variable values, meaning we will be performing **supervised binary classification**. When we choose a model we consider several aspects, such as **number of samples, distribution of samples, number of features, type of features**, etc. Our dataset is slightly imbalanced, consists of 4 categorical features and 1 numerical and the data size is 3700 rows. A good rule of thumb is to always start with a simple model. The first algorithm that I would choose to try out is Logistic Regression. Logistic regression is one of the **most simple** and common algorithms used in ML for binary classification. It is **fast, easy to train and implement**. In our use case the number of features and number of data entries, may not be enough for a deep learning method, but are sufficient for logistic regression. The method also provides **feature importances** and **probability scores** for its predictions. Logistic regression also works well for slightly unbalanced data. In case of severly imbalanced data, we can use the **weighted** variation of the method. A drawback of the logistic regression approach is that data should be **linearly separable**. Another disadvantage is that the algorithm is very **sensitive to outliers** and **vulnerable to overfitting**. Also logistic regression is not cabable of handling a large number of categorical variables. In our case, this might be a little tricky, since "app_category" and "ad_category" have a lot of classes, and each class is interpreted as a single feature from the model, but we still have sufficient samples for each class, so it should not be an obstacle. 

 4. Write up your findings in a Markdown document.
 5. Create a new Github repo and commit your Markdown document there.
```python

```
